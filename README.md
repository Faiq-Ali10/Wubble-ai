# ğŸ§  Wubble AI Agent â€” Internship Technical Assessment

This project is a production-ready FastAPI service designed to handle user prompts using intelligent agents powered by multiple tools. It includes system prompt control, automated evaluation, Docker-based deployment, and conceptual insights into prompt engineering and agent correctness.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ part1/                    # Agent implementation, evaluation framework
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ agent.py
|   |   |â”€â”€tools.py
|   |   |â”€â”€__init__.py
|   |   |â”€â”€test_evaluation.py
â”‚   â”‚   â””â”€â”€ system_prompt.txt
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env                  # (not tracked in Git)
â”‚
â”œâ”€â”€ part2/                    # Docker & deployment
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â””â”€â”€ start.sh
â”‚
â”œâ”€â”€ part3/                    # Conceptual questions
â”‚   â””â”€â”€ ANSWERS.md            # (included below in this README)
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Part 1: FastAPI Agent Implementation

### âœ… Features

- **FastAPI backend** with OpenAPI (Swagger) support.
- **Two tools integrated**:
  - `LangChain + Groq LLM` for reasoning and responses.
  - `SerpAPI` for real-time search-based answers.
- **System prompt** defines tone, constraints, and fallback behavior.
- **Automated test evaluation** using rule-based scoring.
- **Pydantic models** used for all request/response validation.

### ğŸ“¦ Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/Faiq-Ali10/wubble-ai.git
cd wubble-ai
```

2. **Setup virtual environment**

```bash
cd part1
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Create .env file**

```env
GroqAPI=your_groq_api_key
SERPAPI_API_KEY=your_serp_api_key
```

4. **Run the FastAPI app**

```bash
uvicorn app.main:app 
```

5. **Access the docs at http://localhost:8000/docs**

### ğŸ§ª Evaluation Framework

Run the automated test cases with:

```bash
pytest test_evaluation.py
```

The test suite includes:

- Normal queries (weather, math, translations)
- Edge cases (unanswerable questions)
- Off-topic inputs
- Rule-based checks on tone, content, and tool usage

---

## ğŸ³ Part 2: Docker & Deployment

### ğŸ§± Docker Build

From the project root:

```bash
cd part2
docker build -t ai-agent-service .
```

### â–¶ï¸ Run Docker Container

```bash
docker run -d -p 8000:8000 \
  --memory=512m --cpus=1 \
  --env-file ../part1/.env \
  ai-agent-service
```

The `start.sh` script automates the build and run steps.

---

## ğŸ§  Part 3: Conceptual Understanding

### ğŸ“Œ Agent Evaluation & Correctness

**Q1: How would you measure whether your agent is taking the correct action in response to the prompt?**

- By designing test prompts with known, verifiable outputs.
- Using rule-based validators and optionally LLM-as-judge.
- Checking which tool the agent selected, and whether it was contextually appropriate.

**Q2: Propose a mechanism to detect conflicting or stale results.**

- Add a timestamp or freshness check to API responses (e.g., SERP results).
- Compare results from multiple tools and flag if there's significant disagreement.
- Use metadata from the tool response to determine age, source reliability, and consistency.

### âœ¨ Prompt Engineering Proficiency

**Q3: How do you design system prompts to guide agent behavior effectively?**

- Be explicit about the role and tone (e.g., "You are a concise, factual assistant").
- Define fallback behavior ("If unsure, respond with 'I don't know'").
- Set tool usage rules (e.g., "Use SerpAPI only for real-time info").

**Q4: What constraints, tone, and structure do you enforce, and how do you test them?**

**Constraints:**
- Do not hallucinate if the answer is unknown.
- Avoid making up citations.

**Tone:**
- Friendly, concise, and non-repetitive.

**Structure:**
- Short paragraphs or lists.
- Include source hints if using external tools.

**Testing:**
- Include test cases that intentionally force fallback or edge-case responses.
- Manually verify tone for representative samples.
- Use regex or keyword checks for specific structure and fallback validation.

---

## ğŸ“ .gitignore

```gitignore
__pycache__/
*.pyc
.env
venv/
*.log
```

---

## ğŸ‘¤ Author

**Faiq Ali**
- ğŸ“§ faiqalio1oo@gmail.com
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/faiq-ali-83462a255/)

---

## ğŸ“„ License

This repository is created for educational and technical assessment purposes.
